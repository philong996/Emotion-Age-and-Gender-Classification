{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FER2013.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m_h6XIvW8nr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7af5ef37-c98c-479f-9ab0-b967ef46248d"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jt04PdAWllk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "218b8eb6-8708-4a26-cdcf-3d04a96599ce"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gl2u7b-WwPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cpu - gpu configuration\n",
        "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': 56} ) #max: 1 gpu, 56 cpu\n",
        "sess = tf.Session(config=config) \n",
        "keras.backend.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyvqCOl8W1Bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#variables\n",
        "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 32\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zFrkedSXbc0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "35007b2e-da60-4695-eb2b-7b101b8d81cf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrgyqnCPrgZT",
        "colab_type": "text"
      },
      "source": [
        "## Processing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIzqB8RxXMjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1adfbb84-4d49-4430-8251-ca289e465b06"
      },
      "source": [
        "#read kaggle facial expression recognition challenge dataset (fer2013.csv)\n",
        "#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "\n",
        "with open(\"/content/gdrive/My Drive/WeekSeven/fer2013.csv\") as f:\n",
        "    content = f.readlines()\n",
        "\n",
        "lines = np.array(content)\n",
        "\n",
        "num_of_instances = lines.size\n",
        "print(\"number of instances: \",num_of_instances)\n",
        "print(\"instance length: \",len(lines[1].split(\",\")[1].split(\" \")))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of instances:  35888\n",
            "instance length:  2304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-SYK914X1ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize trainset and test set\n",
        "x_train, y_train, x_test, y_test = [], [], [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fweS8oI5X_uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#transfer train and test set data\n",
        "for i in range(1,num_of_instances):\n",
        "    try:\n",
        "        emotion, img, usage = lines[i].split(\",\")\n",
        "          \n",
        "        val = img.split(\" \")\n",
        "            \n",
        "        pixels = np.array(val, 'float32')\n",
        "        \n",
        "        emotion = keras.utils.to_categorical(emotion, num_classes)\n",
        "    \n",
        "        if 'Training' in usage:\n",
        "            y_train.append(emotion)\n",
        "            x_train.append(pixels)\n",
        "        elif 'PublicTest' in usage:\n",
        "            y_test.append(emotion)\n",
        "            x_test.append(pixels)\n",
        "    except:\n",
        "\t      print(\"\",end=\"\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziaBFjxAYDaQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d5e44aac-4532-4ede-cd4c-acf9743f06e4"
      },
      "source": [
        "#data transformation for train and test sets\n",
        "x_train = np.array(x_train, 'float32')\n",
        "y_train = np.array(y_train, 'float32')\n",
        "x_test = np.array(x_test, 'float32')\n",
        "y_test = np.array(y_test, 'float32')\n",
        "\n",
        "x_train /= 255 #normalize inputs between [0, 1]\n",
        "x_test /= 255\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28709 train samples\n",
            "3589 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cuwH0tfiioJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab346b92-4421-40b4-d88a-6d4d476f4d4a"
      },
      "source": [
        "# Convert from 7 columns to 1 column\n",
        "Y_train, Y_test = [], []\n",
        "for i in range(y_train.shape[0]):\n",
        "  for j in range(7):\n",
        "    if 1 == y_train[i][j]:\n",
        "      Y_train.append(j)\n",
        "\n",
        "for i in range(y_test.shape[0]):\n",
        "  for j in range(7):\n",
        "    if 1 == y_test[i][j]:\n",
        "      Y_test.append(j)  \n",
        "\n",
        "Y_train = np.array(Y_train, 'float32')\n",
        "Y_test = np.array(Y_test, 'float32')\n",
        "Y_test.shape, Y_train.shape, Y_train[51], Y_test[101]  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3589,), (28709,), 5.0, 6.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48W0w5UvDoW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#construct CNN structure\n",
        "def CNNd():\n",
        "    model = tf.keras.models.Sequential([\n",
        "            #1st convolution layer\n",
        "            Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1),padding='same'),\n",
        "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            #2nd convolution layer\n",
        "            Conv2D(128, (3, 3), activation='relu',padding='same'),\n",
        "            Conv2D(128, (3, 3), activation='relu',padding='same'),\n",
        "            AveragePooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            #3rd convolution layer\n",
        "            Conv2D(256, (3, 3), activation='relu',padding='same'),\n",
        "            Conv2D(256, (3, 3), activation='relu',padding='same'),\n",
        "            AveragePooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            #4th convolution layer\n",
        "            Conv2D(512, (3, 3), activation='relu',padding='same'),\n",
        "            Conv2D(512, (3, 3), activation='relu',padding='same'),\n",
        "            AveragePooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Flatten(),\n",
        "\n",
        "            #fully connected neural networks\n",
        "            Dense(4096, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Dense(1024, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Dense(num_classes, activation='softmax'),\n",
        "            ])\n",
        "    return model\n",
        "\n",
        "modeld = CNNd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrvvkMjyZCW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ImageDataGenerator for train\n",
        "import math\n",
        "\n",
        "gen = ImageDataGenerator()\n",
        "train_generator = gen.flow(x_train, Y_train, batch_size=batch_size,shuffle=True)\n",
        "test_generator = gen.flow(x_test, Y_test, batch_size=batch_size,shuffle=True)\n",
        "\n",
        "num_steps_train = math.ceil(float(x_train.shape[0])/batch_size)\n",
        "num_steps_val = math.ceil(float(x_test.shape[0])/batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLNSyub7dBmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile model\n",
        "modeld.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTE7BW4VdH3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c60e19c-2018-44e5-c3f0-0967d8f69a12"
      },
      "source": [
        "#Fit model or load weights\n",
        "fit = True\n",
        "\n",
        "if fit == True:\n",
        "\t#model.fit_generator(x_train, y_train, epochs=epochs) #train for all trainset\n",
        "\tmodeld.fit(train_generator, steps_per_epoch=num_steps_train, epochs=25, validation_data=test_generator, validation_steps=num_steps_val) #train for randomly selected one\n",
        "else:\n",
        "\tmodeld.load_weights('/data/facial_expression_model_weights.h5') #load weights"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 898 steps, validate for 113 steps\n",
            "Epoch 1/25\n",
            "898/898 [==============================] - 15s 16ms/step - loss: 2.0181 - accuracy: 0.2237 - val_loss: 1.8409 - val_accuracy: 0.2405\n",
            "Epoch 2/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.8199 - accuracy: 0.2795 - val_loss: 1.8683 - val_accuracy: 0.2767\n",
            "Epoch 3/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.6217 - accuracy: 0.3735 - val_loss: 2.1803 - val_accuracy: 0.2697\n",
            "Epoch 4/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.4082 - accuracy: 0.4631 - val_loss: 1.4344 - val_accuracy: 0.4416\n",
            "Epoch 5/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.2734 - accuracy: 0.5134 - val_loss: 1.3369 - val_accuracy: 0.4976\n",
            "Epoch 6/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.1993 - accuracy: 0.5500 - val_loss: 1.2737 - val_accuracy: 0.5099\n",
            "Epoch 7/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.1396 - accuracy: 0.5727 - val_loss: 1.2500 - val_accuracy: 0.5143\n",
            "Epoch 8/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.0764 - accuracy: 0.5985 - val_loss: 1.2910 - val_accuracy: 0.5146\n",
            "Epoch 9/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 1.0058 - accuracy: 0.6257 - val_loss: 1.1185 - val_accuracy: 0.5882\n",
            "Epoch 10/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.9434 - accuracy: 0.6492 - val_loss: 1.2492 - val_accuracy: 0.5478\n",
            "Epoch 11/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.8555 - accuracy: 0.6881 - val_loss: 1.1765 - val_accuracy: 0.5734\n",
            "Epoch 12/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.7637 - accuracy: 0.7194 - val_loss: 1.5200 - val_accuracy: 0.4656\n",
            "Epoch 13/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.6563 - accuracy: 0.7596 - val_loss: 1.2423 - val_accuracy: 0.5773\n",
            "Epoch 14/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.5504 - accuracy: 0.8008 - val_loss: 1.2989 - val_accuracy: 0.5965\n",
            "Epoch 15/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.4385 - accuracy: 0.8433 - val_loss: 1.4584 - val_accuracy: 0.5926\n",
            "Epoch 16/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.3434 - accuracy: 0.8758 - val_loss: 1.5728 - val_accuracy: 0.5857\n",
            "Epoch 17/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.2712 - accuracy: 0.9026 - val_loss: 1.7056 - val_accuracy: 0.5787\n",
            "Epoch 18/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.2208 - accuracy: 0.9224 - val_loss: 1.9079 - val_accuracy: 0.5801\n",
            "Epoch 19/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1837 - accuracy: 0.9340 - val_loss: 1.7924 - val_accuracy: 0.6082\n",
            "Epoch 20/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1587 - accuracy: 0.9449 - val_loss: 2.3805 - val_accuracy: 0.5397\n",
            "Epoch 21/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1425 - accuracy: 0.9517 - val_loss: 2.1297 - val_accuracy: 0.6066\n",
            "Epoch 22/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1239 - accuracy: 0.9562 - val_loss: 2.0568 - val_accuracy: 0.5946\n",
            "Epoch 23/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1201 - accuracy: 0.9572 - val_loss: 2.3503 - val_accuracy: 0.5795\n",
            "Epoch 24/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1080 - accuracy: 0.9626 - val_loss: 2.2041 - val_accuracy: 0.6018\n",
            "Epoch 25/25\n",
            "898/898 [==============================] - 13s 14ms/step - loss: 0.1076 - accuracy: 0.9624 - val_loss: 2.4244 - val_accuracy: 0.5901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v4C6FZWlJT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modeld.save('/content/gdrive/My Drive/WeekSeven/emotiond.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd4V66RfTxMH",
        "colab_type": "text"
      },
      "source": [
        "## Testing our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjNpxy_iqQ8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "0c802a73-000a-4ab7-91c7-1105df55220f"
      },
      "source": [
        "!pip install google_images_download"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_images_download\n",
            "  Downloading https://files.pythonhosted.org/packages/18/ed/0319d30c48f3653802da8e6dcfefcea6370157d10d566ef6807cceb5ec4d/google_images_download-2.8.0.tar.gz\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /tensorflow-2.1.0/python3.6 (from selenium->google_images_download) (1.25.7)\n",
            "Building wheels for collected packages: google-images-download\n",
            "  Building wheel for google-images-download (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-images-download: filename=google_images_download-2.8.0-py2.py3-none-any.whl size=14550 sha256=67b0d49d84c463d74707ed26dfaf8f67c595e9ab6231267a7f2e419f62615ef1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/28/ad/f56e7061e1d2a9a1affe2f9c649c2570cb9198dd24ede0bbab\n",
            "Successfully built google-images-download\n",
            "Installing collected packages: selenium, google-images-download\n",
            "Successfully installed google-images-download-2.8.0 selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXqVlgatppTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google_images_download import google_images_download \n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55IK_VtbqPgL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "3429f0eb-ff78-488d-825b-e5614fe0997e"
      },
      "source": [
        "# the limit of google_images_download without chromedriver is 100 item per keywords \n",
        "response = google_images_download.googleimagesdownload()\n",
        "\n",
        "arguments = [{\"keywords\"     : 'asian male disgust face',\n",
        "             \"limit\"        : 5,\n",
        "             \"print_urls\"   : True,\n",
        "             \"size\"         : \">2MP\",},\n",
        "             \n",
        "             {\"keywords\"     : 'asian female disgust face',\n",
        "             \"limit\"        : 5,\n",
        "             \"print_urls\"   : True,\n",
        "             \"size\"         : \">2MP\"}]\n",
        "for argument in arguments: \n",
        "    paths = response.download(argument)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Item no.: 1 --> Item name = asian male disgust face\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "Image URL: https://marketplace.canva.com/MADaExVtxkE/1/screen_2x/canva-headshot-photo-of-asian-man-with-hate-and-disgusting-face.-on-grey-background.-MADaExVtxkE.jpg\n",
            "Completed Image ====> 1.canva-headshot-photo-of-asian-man-with-hate-and-disgusting-face.-on-grey-background.-MADaExVtxkE.jpg\n",
            "Image URL: https://st4.depositphotos.com/5192617/21296/i/1600/depositphotos_212968022-stock-photo-headshot-photo-asian-man-hate.jpg\n",
            "Completed Image ====> 2.depositphotos_212968022-stock-photo-headshot-photo-asian-man-hate.jpg\n",
            "Image URL: https://thumbs.dreamstime.com/z/funny-asian-man-lazy-bored-mocking-face-portrait-young-attractive-showing-disgust-unhappy-expression-145456178.jpg\n",
            "Completed Image ====> 3.funny-asian-man-lazy-bored-mocking-face-portrait-young-attractive-showing-disgust-unhappy-expression-145456178.jpg\n",
            "Image URL: https://video-images.vice.com/articles/5b5b68f8d736970006757f02/lede/1532721082758-wenwen.jpeg?crop=0.9795555555555555xw%3A1xh%3Bcenter%2Ccenter&resize=2000%3A*\n",
            "Completed Image ====> 4.1532721082758-wenwen.jpeg\n",
            "Image URL: https://miro.medium.com/max/4000/0*qk9hP87Ya386N7U5.jpg\n",
            "Completed Image ====> 5.0*qk9hP87Ya386N7U5.jpg\n",
            "\n",
            "Errors: 0\n",
            "\n",
            "\n",
            "Item no.: 1 --> Item name = asian female disgust face\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "Image URL: https://st4.depositphotos.com/1049680/21148/i/1600/depositphotos_211489942-stock-photo-young-asian-woman-isolated-background.jpg\n",
            "Completed Image ====> 1.depositphotos_211489942-stock-photo-young-asian-woman-isolated-background.jpg\n",
            "Image URL: https://st4.depositphotos.com/1049680/21913/i/1600/depositphotos_219137146-stock-photo-young-asian-woman-isolated-background.jpg\n",
            "Completed Image ====> 2.depositphotos_219137146-stock-photo-young-asian-woman-isolated-background.jpg\n",
            "Image URL: https://thumbs.dreamstime.com/z/beautiful-middle-aged-woman-crossed-arms-twists-her-face-disgust-asian-shows-disapproval-twisting-lips-crossing-147824705.jpg\n",
            "Completed Image ====> 3.beautiful-middle-aged-woman-crossed-arms-twists-her-face-disgust-asian-shows-disapproval-twisting-lips-crossing-147824705.jpg\n",
            "Image URL: https://st4.depositphotos.com/1049680/21055/i/1600/depositphotos_210554880-stock-photo-young-chinese-woman-isolated-background.jpg\n",
            "Completed Image ====> 4.depositphotos_210554880-stock-photo-young-chinese-woman-isolated-background.jpg\n",
            "Image URL: https://thumbs.dreamstime.com/z/portrait-young-shocked-surprised-beautiful-asian-korean-woman-covering-mouth-hand-disbelief-surprise-face-136248484.jpg\n",
            "Completed Image ====> 5.portrait-young-shocked-surprised-beautiful-asian-korean-woman-covering-mouth-hand-disbelief-surprise-face-136248484.jpg\n",
            "\n",
            "Errors: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoU5ggf6dWov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for drawing bar chart for emotion preditions\n",
        "def emotion_analysis(emotions):\n",
        "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "    y_pos = np.arange(len(objects))\n",
        "    \n",
        "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
        "    plt.xticks(y_pos, objects)\n",
        "    plt.ylabel('percentage')\n",
        "    plt.title('emotion')\n",
        "    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13-q6hthiuQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "monitor_testset_results = False\n",
        "\n",
        "if monitor_testset_results == True:\n",
        "\t#make predictions for test set\n",
        "\tpredictions = model.predict(x_test)\n",
        "\n",
        "\tindex = 0\n",
        "\tfor i in predictions:\n",
        "\t\tif index < 30 and index >= 20:\n",
        "\t\t\t#print(i) #predicted scores\n",
        "\t\t\t#print(y_test[index]) #actual scores\n",
        "\t\t\t\n",
        "\t\t\ttesting_img = np.array(x_test[index], 'float32')\n",
        "\t\t\ttesting_img = testing_img.reshape([48, 48]);\n",
        "\t\t\t\n",
        "\t\t\tplt.gray()\n",
        "\t\t\tplt.imshow(testing_img)\n",
        "\t\t\tplt.show()\n",
        "\t\t\t\n",
        "\t\t\tprint(i)\n",
        "\t\t\t\n",
        "\t\t\temotion_analysis(i)\n",
        "\t\t\tprint(\"----------------------------------------------\")\n",
        "\t\tindex = index + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FYW-YMkmxuT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c28680d1-e1e7-42d4-e278-9c85d3fd4a1d"
      },
      "source": [
        "# We show how the library load our image\n",
        "img = image.load_img(\"/content/gdrive/My Drive/WeekSeven/KA.DI2.43.tiff\", grayscale=True, target_size=(48, 48))\n",
        "img"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAAAAAByaaZbAAAHK0lEQVR4nAXBSYxlVRkA4PP/5z/n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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=48x48 at 0x7FE4D03531D0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Iaflvxma6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "75c70ec6-b98c-4f76-f738-749ba09014b6"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "img = image.load_img(\"./downloads/asian female disgust face/5.portrait-young-shocked-surprised-beautiful-asian-korean-woman-covering-mouth-hand-disbelief-surprise-face-136248484.jpg\", grayscale=True, target_size=(48, 48))\n",
        "#Convert to array\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis = 0)\n",
        "#Normalize\n",
        "x /= 255\n",
        "#Predict\n",
        "custom = model.predict(x)\n",
        "emotion_analysis(custom[0])\n",
        "\n",
        "x = np.array(x, 'float32')\n",
        "x = x.reshape([48, 48]);\n",
        "\n",
        "plt.gray()\n",
        "plt.imshow(x)\n",
        "plt.show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAXlklEQVR4nO3debhkdX3n8feHbUDBRu2OCZsN2mqQ\nMS4ti8sMLigyCk7ECMIoysjjTHAd5xEjIoPLaMwTTUZcmshDABVBB+2YVoJEotGgNCD7QDoNSKOR\nBmWXsH3nj3MuFLfr3lt9+566NOf9ep567ll+depb556qT53fOacqVYUkqb82me8CJEnzyyCQpJ4z\nCCSp5wwCSeo5g0CSes4gkKSeMwikMUjynSRvnu86pGHidQTS3EpyLPDUqjp0vmuRRuEegST1nEGg\nXkmyXZJvJFmb5Jok72ynH5vkjCSnJrk9yaVJnpbkA0luTHJ9kldMWs7yJL9OsirJ29rp+wJ/Arwh\nyR1JLm6nn5vkv7bDmyQ5Osl17bJPTrKgnbc4SSV5c5KfJ7kpyQfHvZ7ULwaBeiPJJsDfABcD2wMv\nA96d5JVtk9cApwCPBy4CzqJ5jWwPHAd8cWBxpwFrgO2AA4GPJ3lpVX0X+Djwtarauqr+YEgph7W3\nlwC7AFsDn53U5kXA09saj0ny+7N+4tIMDAL1yfOBRVV1XFXdU1WrgROAg9r5P6yqs6rqPuAMYBHw\niaq6l+aNf3GSbZPsCLwQeH9V3V1VPwP+CnjTiHUcAvx5Va2uqjuADwAHJdlsoM3/qqrfVtXFNME1\nLFCkObHZzE2kR40nA9sluWVg2qbAD4HrgF8NTP8tcFNV3T8wDs2n9+2AX1fV7QPtrwOWjljHdm37\nwftuBjxpYNq/Dgzf1T6u1An3CNQn1wPXVNW2A7dtqmq/9VzOL4AnJNlmYNpOwA3t8Eyn4v2CJpQG\n73sfDw8iaWwMAvXJT4Hbk7w/yVZJNk2yW5Lnr89Cqup64MfA/06yZZJnAYcDp7ZNfkXTjTTV6+ur\nwHuS7Jxkax46pnDfrJ6VtIEMAvVG283zauDZwDXATTR9+wtmsbiDgcU0n+7PBD5cVd9r553R/r05\nyYVD7nsizUHpH7R13A28YxY1SHPCC8okqefcI5CknjMIJKnnDAJJ6jmDQJJ6rrMLypKcSHOGxo1V\ntduQ+QH+AtiP5oKZw6pq2BkWD7Nw4cJavHjxHFcrSY9uF1xwwU1VtWjYvC6vLD6J5vtTTp5i/quA\nJe1tD+Dz7d9pLV68mJUrV85RiZLUD0mum2peZ11DVfUD4NfTNDkAOLka5wHbJvm9ruqRJA03n8cI\ntqe55H/CmnbaOpIckWRlkpVr164dS3GS1BcbxcHiqlpWVUuraumiRUO7uCRJszSfQXADsOPA+A48\n9KVdkqQxmc8gWA68KY09gVur6pfzWI8k9VKXp49+FdgbWJhkDfBhYHOAqvoCsILm1NFVNKePvqWr\nWiRJU+ssCKrq4BnmF/DHXT2+JGk0G8XBYklSdwwCSeo5f7NY0lh8+uyr57uEh3nPPk+b7xIeMdwj\nkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4g\nkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4g\nkKSeMwgkqecMAknqOYNAknrOIJCknjMIJKnnDAJJ6rlOgyDJvkmuSrIqyVFD5u+U5PtJLkpySZL9\nuqxHkrSuzoIgyabA8cCrgF2Bg5PsOqnZ0cDpVfUc4CDgc13VI0karss9gt2BVVW1uqruAU4DDpjU\npoDHtcMLgF90WI8kaYgug2B74PqB8TXttEHHAocmWQOsAN4xbEFJjkiyMsnKtWvXdlGrJPXWfB8s\nPhg4qap2APYDTkmyTk1VtayqllbV0kWLFo29SEl6NOsyCG4AdhwY36GdNuhw4HSAqvonYEtgYYc1\nSZIm6TIIzgeWJNk5yRY0B4OXT2rzc+BlAEl+nyYI7PuRpDHqLAiq6j7gSOAs4Eqas4MuT3Jckv3b\nZv8DeFuSi4GvAodVVXVVkyRpXZt1ufCqWkFzEHhw2jEDw1cAL+yyBknS9Ob7YLEkaZ4ZBJLUcwaB\nJPWcQSBJPWcQSFLPGQSS1HMGgST1nEEgST1nEEhSzxkEktRzBoEk9ZxBIEk9ZxBIUs8ZBJLUcwaB\nJPWcQSBJPWcQSFLPGQSS1HMGgST1nEEgST1nEEhSzxkEktRzBoEk9ZxBIEk9ZxBIUs8ZBJLUcwaB\nJPWcQSBJPWcQSFLPGQSS1HMGgST13MhBkORFSd7SDi9KsnN3ZUmSxmWkIEjyYeD9wAfaSZsDp3ZV\nlCRpfEbdI/jPwP7AnQBV9Qtgm66KkiSNz6hBcE9VFVAASR47yp2S7JvkqiSrkhw1RZs/SnJFksuT\nfGXEeiRJc2SzEdudnuSLwLZJ3ga8FThhujsk2RQ4HtgHWAOcn2R5VV0x0GYJTXfTC6vqN0l+ZzZP\nQpI0eyMFQVX9WZJ9gNuApwPHVNXZM9xtd2BVVa0GSHIacABwxUCbtwHHV9Vv2se5cT3rlyRtoFH3\nCGjf+Gd68x+0PXD9wPgaYI9JbZ4GkORHwKbAsVX13ckLSnIEcATATjvttB4lSJJmMupZQ7cnuW3S\n7fokZybZZQMefzNgCbA3cDBwQpJtJzeqqmVVtbSqli5atGgDHk6SNNmoewSfoflE/xUgwEHAU4AL\ngRNp3sgnuwHYcWB8h3baoDXAT6rqXuCaJFfTBMP5I9YlSdpAo541tH9VfbGqbq+q26pqGfDKqvoa\n8Pgp7nM+sCTJzkm2oAmP5ZPafJM2RJIspOkqWr2+T0KSNHujBsFd7Wmem7S3PwLubufVsDtU1X3A\nkcBZwJXA6VV1eZLjkuzfNjsLuDnJFcD3gf9ZVTfP+tlIktbbqF1DhwB/AXyO5o3/PODQJFvRvNkP\nVVUrgBWTph0zMFzAe9ubJGkejHr66GrgNVPM/se5K0eSNG4jBUGSLYHDgWcCW05Mr6q3dlSXJGlM\nRj1GcArwu8ArgX+gOQPo9q6KkiSNz6hB8NSq+hBwZ1X9NfCfWPfiMEnSRmjUILi3/XtLkt2ABYDf\nCyRJjwKjnjW0LMnjgaNprgXYGvhQZ1VJksZm1CA4p/1iuB8AuwD4C2WS9OgwatfQN4ZM+/pcFiJJ\nmh/T7hEkeQbNKaMLkvzhwKzHMXAaqSRp4zVT19DTgVcD2/LwC8pup/ktAUnSRm7aIKiqbwHfSrJX\nVf3TmGqSJI3RqAeLVyX5E2Dx4H28sliSNn6jBsG3gB8C3wPu764cSdK4jRoEj6mq93daiSRpXox6\n+ui3k+zXaSWSpHkxahC8iyYM7m5/r/j2JLd1WZgkaTxG/T2CbbouRJI0P0baI0jj0CQfasd3TLJ7\nt6VJksZh1K6hzwF7AW9sx+8Aju+kIknSWI161tAeVfXcJBcBVNVvkmzRYV2SpDEZ+fcIkmxK88P1\nJFkEPNBZVZKksRk1CP4SOBP4nSQfo/nB+o93VpUkaWxGPWvoy0kuAF4GBHhtVV3ZaWWSpLEYKQiS\n7AlcXlXHt+OPS7JHVf2k0+okSZ0btWvo8zRnCk24o50mSdrIjRoEqaqaGKmqBxj9jCNJ0iPYqEGw\nOsk7k2ze3t4FrO6yMEnSeIwaBG8HXgDcAKwB9gCO6KooSdL4zNi9014/cEhVHTSGeiRJYzbjHkFV\n3Q8cPIZaJEnzYNQDvj9K8lnga8CdExOr6sJOqpIkjc2oQfDs9u9xA9MKeOncliNJGrdRryx+SdeF\nSJLmx6i/R/CkJF9K8p12fNckh3dbmiRpHEY9ffQk4Cxgu3b8auDdM90pyb5JrkqyKslR07R7XZJK\nsnTEeiRJc2TUIFhYVafTfvV0Vd0H3D/dHdrTTo8HXgXsChycZNch7bah+U1kv7dIkubBqEFwZ5In\n8tDvEewJ3DrDfXYHVlXV6qq6BzgNOGBIu48AnwTuHrEWSdIcGjUI3gssB3ZJ8iPgZOAdM9xne+D6\ngfE17bQHJXkusGNV/e10C0pyRJKVSVauXbt2xJIlSaMY9fTRK2h+mOYu4HbgmzTHCWYtySbAnwOH\nzdS2qpYBywCWLl1aMzSXJK2HUfcITgaeQfOrZP8HeBpwygz3uQHYcWB8h3bahG2A3YBzk1wL7Aks\n94CxJI3XqHsEu1XV4IHe7ye5Yob7nA8sSbIzTQAcBLxxYmZV3QosnBhPci7wvqpaOWJNkqQ5MOoe\nwYXtAWIAkuwBTPuG3Z5ZdCTNaadXAqdX1eVJjkuy/2wLliTNrVH3CJ4H/DjJz9vxnYCrklwKVFU9\na9idqmoFsGLStGOmaLv3iLVIkubQqEGwb6dVSJLmzajfNXRd14VIkubHqMcIJEmPUgaBJPWcQSBJ\nPWcQSFLPGQSS1HMGgST1nEEgST1nEEhSzxkEktRzBoEk9ZxBIEk9ZxBIUs8ZBJLUcwaBJPWcQSBJ\nPWcQSFLPGQSS1HMGgST1nEEgST1nEEhSzxkEktRzBoEk9ZxBIEk9ZxBIUs8ZBJLUcwaBJPWcQSBJ\nPWcQSFLPGQSS1HMGgST1nEEgST1nEEhSz3UaBEn2TXJVklVJjhoy/71JrkhySZJzkjy5y3okSevq\nLAiSbAocD7wK2BU4OMmuk5pdBCytqmcBXwf+tKt6JEnDdblHsDuwqqpWV9U9wGnAAYMNqur7VXVX\nO3oesEOH9UiShugyCLYHrh8YX9NOm8rhwHeGzUhyRJKVSVauXbt2DkuUJD0iDhYnORRYCnxq2Pyq\nWlZVS6tq6aJFi8ZbnCQ9ym3W4bJvAHYcGN+hnfYwSV4OfBD4j1X1bx3WI0kaoss9gvOBJUl2TrIF\ncBCwfLBBkucAXwT2r6obO6xFkjSFzoKgqu4DjgTOAq4ETq+qy5Mcl2T/ttmngK2BM5L8LMnyKRYn\nSepIl11DVNUKYMWkaccMDL+8y8eXJM3sEXGwWJI0fwwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNA\nknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4gkKSeMwgkqecMAknqOYNA\nknrOIJCknjMIJKnnDAJJ6jmDQJJ6ziCQpJ4zCCSp5wwCSeo5g0CSes4gkKSeMwgkqecMAknquc3m\nuwA9+nz67Kvnu4SHec8+T5vvEqRHNPcIJKnnDAJJ6jmDQJJ6rtMgSLJvkquSrEpy1JD5/y7J19r5\nP0myuMt6JEnr6iwIkmwKHA+8CtgVODjJrpOaHQ78pqqeCnwa+GRX9UiShutyj2B3YFVVra6qe4DT\ngAMmtTkA+Ot2+OvAy5Kkw5okSZN0efro9sD1A+NrgD2malNV9yW5FXgicNNgoyRHAEe0o3ckuaqT\nike3kEk1bgR6W/N756CQEfV2HY/ZxrZdwCNjPT95qhkbxXUEVbUMWDbfdUxIsrKqls53HevDmru3\nsdUL1jwuj/Sau+waugHYcWB8h3ba0DZJNgMWADd3WJMkaZIug+B8YEmSnZNsARwELJ/UZjnw5nb4\nQODvq6o6rEmSNElnXUNtn/+RwFnApsCJVXV5kuOAlVW1HPgScEqSVcCvacJiY/CI6aZaD9bcvY2t\nXrDmcXlE1xw/gEtSv3llsST1nEEgST1nEGxkkhyb5H1Jjkvy8jE83muHXBE+F8t9Z5Irk3x5rpe9\noZIsTnLZfNcxnzbGdZBkRZJt57uOqbTr9I2zvO8dc13PIINgjrWnwXauqo6pqu+N4aFeS/MVIXPt\nvwP7VNUhs13AuNa15seo/980Nqmq/arqlq7r2gCLgaFBMN/bcu+DIMk3k1yQ5PL2CmaS3JHkY0ku\nTnJekie105/Sjl+a5KMTKZ1k7yQ/TLIcuKL9tP7ugcf4WJJ3bUCNH0xydZJ/BJ7eTjspyYHt8CeS\nXJHkkiR/NkKt3x5Y9meTHDZsOUleAOwPfCrJz5I8ZbbPYdLz+QKwC/Cd9rmdmOSnSS5KckDbZnG7\nTi9sby8YqP/BdT0X9Uxh0yQntNvF3yXZKsnbkpzfbhffSPKYtqaTknwhycr2//TqdvphSb6V5Nwk\n/5zkw+30Od0+ppPksUn+tq35siRvSHJM+zwuS7Isab7WJcnz2nYXA3/ccQ3XJlnYzl+a5Nx2+Ngk\npyT5Ec0ZhVOtw8VpvtDyZOAyYMeJZQ57vIHn9w/t6/2sJL83Yv2L0+y9Tt4enpLku+3yfpjkGW37\nB1+b7fjEp/lPAC9uX0vvaZ/b8iR/D5yTZOsk57Tb+6UTr4WxqKpe34AntH+3otmgnggU8Jp2+p8C\nR7fD3wYOboffDtzRDu8N3Ans3I4vBi5shzcB/gV44izrex5wKfAY4HHAKuB9wEk01148EbiKh84A\n23aEWr89sPzPAodNs5yTgAM7WO/X0lx2/3Hg0InHBK4GHts+3y3b6UtoTjleZ113tE0sBu4Dnt2O\nnw4cOvg/BD4KvGNgHX23/V8vofk6lS3b9frLdt1ObF9L53L7GOG5vA44YWB8wcQ2346fMrCtXwL8\nh3b4U8BlHdZwLbCwHV8KnNsOHwtcAGzVjk+3Dh8A9hyyTQ17vM2BHwOL2mlvoDmlfUO2h3OAJe20\nPWiug1rnNcPUr73D2m1l4j1oM+Bx7fBCmtd6BpfR1a33ewTAO9tPQOfRXOW8BLiH5o0Umo1ycTu8\nF3BGO/yVScv5aVVdA1BV1wI3J3kO8Argoqqa7RXTLwbOrKq7quo21r0o71bgbuBLSf4QuGuEWoeZ\najldewVwVJKfAefSvIHuRPPCPSHJpTTPY7B76sF13aFrqupn7fDENrBb+8nvUuAQ4JkD7U+vqgeq\n6p+B1cAz2ulnV9XNVfVb4P8CL5rj7WMmlwL7JPlkkhdX1a3AS9J87fulwEuBZ6bpW9+2qn7Q3u+U\njmuYzvJ2fU1YZx2206+rqvNGfLynA7sBZ7fb2tE033YwqmHbwwuAM9rlfREYaQ9jkrOr6tftcICP\nJ7kE+B7Nd7E9aRbLXG+97mNNsjfwcmCvqrqr3T3dEri32hgG7me09XTnpPG/okn83wVOnIt6h6nm\nwr3dgZfR7CEcSfPinsp9PLxLcMtZLmeuBHhdVT3siwSTHAv8CviDtt67B2ZPXtdd+LeB4ftpPo2e\nBLy2qi5O052290CbyRfk1AzTx7V9XJ3kucB+wEeTnEPT7bO0qq5v1/OWXT3+NDUMboeTH3/y/3eq\ndTh0O5ji8c4ELq+qvWb5NCZvD08CbqmqZw9p++BzS7IJsMU0yx18DocAi4DnVdW9Sa6l4//NhL7v\nESyg+T2Eu9r+vT1naH8ezW4nzHwV9JnAvsDzaa6unq0fAK9t+yS3AV4zODPJ1sCCqloBvIfmjXO6\nWq8Ddk3zo0Db0rzxT7ec24FtNqD+mZwFvGOgn/o57fQFwC+r6gHgv9BcnT7ftgF+mWRzmhftoNcn\n2STNcZRdaLrZoPlk+oQkW9EceP9RO32uto9pJdkOuKuqTqXp7nluO+um9n9+IEA1B1lvSTLxaXvW\nB/FHrOFamm5PeGg7ncpU63B9Hu8qYFGSvdo2myd55jSLmcltwDVJXt8uL0kmXjPX8tBz259m7xZm\nfi0tAG5sQ+AlTPNtoXOt13sENP26b09yJc2GMmw3c9C7gVOTfLC975S7uFV1T5Lv03xquH+2BVbV\nhUm+BlwM3EjzHU6DtgG+lWRLmk/XE9+uO7TW9lPg6TR9rdcAF82wnNNoumjeSdPv+S+zfS5T+Ajw\nGeCS9tPTNcCrgc8B30jyprb+cewFzORDwE+Ate3fwRf1z4Gf0hzHeXtV3d1m20+Bb9B0Q5xaVSth\n7raPEfx7moP9DwD3Av+N5s30MuBfefj29BbgxCQF/F3HNWxF0w35EZouwemssw4z/a8ZrvN47fo+\nEPjLJAto3vs+A1w+62fVhOXnkxxN82Z/Gs3r9ASa19LFPHzbvQS4v51+EvCbScv7MvA3bZfdSuD/\nbUBt68WvmFgPac4S+W1VVZKDaA7GDj2y376pXQi8vu03Hqv1qVUbJslJNAcBvz5p+mE0XTBHDrnP\nvG4fG4vp1qHmTt/3CNbX84DPtt0YtwBvHdYozQVY36Y5yDtfL/KRatX4PUK2D+lB7hFIUs/1/WCx\nJPWeQSBJPWcQSFLPGQSS1HMGgST13P8H9lxdusqdWF0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPVgw6Ita2n",
        "colab_type": "text"
      },
      "source": [
        "## Try a different model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6DBdCZ7UTr1",
        "colab_type": "text"
      },
      "source": [
        "Different models that we have tried"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuO06r5PY8aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#construct CNN structure\n",
        "def CNN():\n",
        "    model = tf.keras.models.Sequential([\n",
        "            #1st convolution layer\n",
        "            Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)),\n",
        "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
        "\n",
        "            #2nd convolution layer\n",
        "            Conv2D(64, (3, 3), activation='relu'),\n",
        "            Conv2D(64, (3, 3), activation='relu'),\n",
        "            AveragePooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "\n",
        "            #3rd convolution layer\n",
        "            Conv2D(128, (3, 3), activation='relu'),\n",
        "            Conv2D(128, (3, 3), activation='relu'),\n",
        "            AveragePooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "\n",
        "            Flatten(),\n",
        "\n",
        "            #fully connected neural networks\n",
        "            Dense(1024, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1024, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(num_classes, activation='softmax'),\n",
        "            ])\n",
        "    return model\n",
        "\n",
        "model = CNN()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iKPjpiiqx6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNNb():\n",
        "    model = tf.keras.models.Sequential([\n",
        "            #1st convolution layer\n",
        "            Conv2D(64, (3, 3), activation='relu', input_shape=(48,48,1)),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "\n",
        "            #2nd convolution layer\n",
        "            Conv2D(128, (3, 3), activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "\n",
        "            #3rd convolution layer\n",
        "            Conv2D(256, (3, 3), activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(3,3), strides=(2, 2)),\n",
        "       \n",
        "            #fully connected neural networks\n",
        "            Dropout(0.2),\n",
        "            Dense(4096, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1024, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            Dense(num_classes, activation='softmax'),\n",
        "            ])\n",
        "    return model\n",
        "\n",
        "modelb = CNNb()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX8DdXdhvcou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile model\n",
        "modelb.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrhiuhrFvRd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelb.fit(train_generator, steps_per_epoch=num_steps_train, epochs=20, validation_data=test_generator, validation_steps=num_steps_val) #train for randomly selected one\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vn8uz9Fva4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}